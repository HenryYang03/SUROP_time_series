{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# GRU Model with Decay Mechanism",
   "id": "a51119d7382670f1"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-17T22:55:38.096612Z",
     "start_time": "2024-08-17T22:55:38.093712Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T22:55:38.657910Z",
     "start_time": "2024-08-17T22:55:38.116703Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the dataset\n",
    "train_x = pd.read_csv('/Users/mohanyang/Documents/GitHub/SUROP_time_series/SUROP/data/train_x.csv')\n",
    "train_y = pd.read_csv('/Users/mohanyang/Documents/GitHub/SUROP_time_series/SUROP/data/train_y.csv')\n",
    "test_x = pd.read_csv('/Users/mohanyang/Documents/GitHub/SUROP_time_series/SUROP/data/test_x.csv')"
   ],
   "id": "11206d8a00b7cdcc",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T22:55:38.666684Z",
     "start_time": "2024-08-17T22:55:38.658880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Display the first few rows\n",
    "print(train_x.head())\n",
    "print(train_x.shape)"
   ],
   "id": "8848f19ab8badcaf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Hour  ID    Age  Gender  Unit1  Unit2     HR   O2Sat   Temp     SBP  ...  \\\n",
      "0     1   2  66.67     1.0    0.0    1.0  74.78  100.06  35.61  121.68  ...   \n",
      "1     2   2  66.67     1.0    0.0    1.0  74.78  100.06  35.61  121.68  ...   \n",
      "2     3   2  66.67     1.0    0.0    1.0  74.78  100.06  35.61  121.68  ...   \n",
      "3     4   2  66.67     1.0    0.0    1.0  74.78  100.06  35.61  121.68  ...   \n",
      "4     5   2  66.67     1.0    0.0    1.0  84.42   99.58  35.61  114.60  ...   \n",
      "\n",
      "   Phosphate  Potassium  Bilirubin_total  TroponinI    Hct   Hgb  PTT   WBC  \\\n",
      "0        NaN       4.48              NaN        NaN  28.83  9.21  NaN  8.15   \n",
      "1        NaN       4.48              NaN        NaN  28.83  9.21  NaN  8.15   \n",
      "2        NaN       4.48              NaN        NaN  28.83  9.21  NaN  8.15   \n",
      "3        NaN       4.48              NaN        NaN  28.83  9.21  NaN  8.15   \n",
      "4        NaN       4.48              NaN        NaN  28.83  9.21  NaN  8.15   \n",
      "\n",
      "   Fibrinogen  Platelets  \n",
      "0         NaN        NaN  \n",
      "1         NaN        NaN  \n",
      "2         NaN        NaN  \n",
      "3         NaN        NaN  \n",
      "4         NaN        NaN  \n",
      "\n",
      "[5 rows x 40 columns]\n",
      "(482010, 40)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T22:55:38.669320Z",
     "start_time": "2024-08-17T22:55:38.667295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(train_y.head())\n",
    "print(train_y.shape)"
   ],
   "id": "780add8981ad9955",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID  Outcome\n",
      "0   2        0\n",
      "1   3        0\n",
      "2   5        1\n",
      "3   6        0\n",
      "4  13        0\n",
      "(12115, 2)\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T22:55:38.678532Z",
     "start_time": "2024-08-17T22:55:38.670424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "max_hours = train_x.groupby('ID')['Hour'].count().max()\n",
    "print(f'Maximum number of hours: {max_hours}')"
   ],
   "id": "48f42e6abd239289",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum number of hours: 336\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T22:55:38.683510Z",
     "start_time": "2024-08-17T22:55:38.679068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get unique person IDs\n",
    "person_ids = train_x['ID'].unique()\n",
    "\n",
    "# Initialize tensors for features, masks, and time intervals\n",
    "num_persons = len(person_ids)\n",
    "num_features = train_x.shape[1] - 2  # excluding 'ID' and 'Hour' columns"
   ],
   "id": "64be0cb97c0c0bf3",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T22:55:38.686040Z",
     "start_time": "2024-08-17T22:55:38.684034Z"
    }
   },
   "cell_type": "code",
   "source": "person_ids",
   "id": "4a7175c1c823f814",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    2,     3,     5, ..., 21630, 21632, 21633])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T22:55:38.688013Z",
     "start_time": "2024-08-17T22:55:38.686564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Initialize tensors\n",
    "X_tensor = np.zeros((num_persons, max_hours, num_features))\n",
    "M_tensor = np.zeros((num_persons, max_hours, num_features))\n",
    "Delta_tensor = np.zeros((num_persons, max_hours, num_features))"
   ],
   "id": "845478173d527d86",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T22:55:42.716671Z",
     "start_time": "2024-08-17T22:55:38.688581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i, person_id in enumerate(person_ids):\n",
    "    person_data = train_x[train_x['ID'] == person_id].sort_values('Hour')\n",
    "    hours = person_data['Hour'].values\n",
    "    features = person_data.drop(columns=['ID', 'Hour']).values\n",
    "    \n",
    "    # Fill X_tensor\n",
    "    X_tensor[i, :len(hours), :] = features\n",
    "    \n",
    "    # Fill M_tensor (masking)\n",
    "    M_tensor[i, :len(hours), :] = ~np.isnan(features)\n",
    "    \n",
    "    # Fill Delta_tensor (time intervals)\n",
    "    for t in range(1, len(hours)):\n",
    "        for f in range(num_features):\n",
    "            Delta_tensor[i, t, f] = (hours[t] - hours[t-1]) + Delta_tensor[i, t-1, f] * (1 - M_tensor[i, t-1, f])"
   ],
   "id": "bfd4c27fb4e6951a",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[18], line 15\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mlen\u001B[39m(hours)):\n\u001B[1;32m     14\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_features):\n\u001B[0;32m---> 15\u001B[0m         Delta_tensor[i, t, f] \u001B[38;5;241m=\u001B[39m (hours[t] \u001B[38;5;241m-\u001B[39m hours[t\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]) \u001B[38;5;241m+\u001B[39m Delta_tensor[i, t\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, f] \u001B[38;5;241m*\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m M_tensor[i, t\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, f])\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-17T22:55:42.717828Z",
     "start_time": "2024-08-17T22:55:42.717783Z"
    }
   },
   "cell_type": "code",
   "source": "X_tensor",
   "id": "cb9e167e59aed66a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "M_tensor",
   "id": "a9a26ac8cc092b93",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "Delta_tensor",
   "id": "55636ca501ff55ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Replace NaNs with zeros in X_tensor (not necessary in M_tensor and Delta_tensor)\n",
    "X_tensor = np.nan_to_num(X_tensor)\n",
    "\n",
    "print(X_tensor.shape, M_tensor.shape, Delta_tensor.shape)"
   ],
   "id": "91fa8ed6cceb3e8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Ensure train_y is aligned with X_tensor\n",
    "labels = np.zeros(len(person_ids))\n",
    "\n",
    "for i, person_id in enumerate(person_ids):\n",
    "    if person_id in train_y['ID'].values:\n",
    "        labels[i] = train_y[train_y['ID'] == person_id]['Outcome'].values[0]\n",
    "\n",
    "labels = labels.reshape(-1, 1)\n",
    "print(labels.shape)"
   ],
   "id": "41512dbd553e7a9c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Split the data into training and test sets\n",
    "X_train, X_test, M_train, M_test, Delta_train, Delta_test, y_train, y_test = train_test_split(\n",
    "    X_tensor, M_tensor, Delta_tensor, labels, test_size=0.2, random_state=10\n",
    ")"
   ],
   "id": "aa7a4b86aa34fa65",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class GRUD(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(GRUD, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # GRU weights\n",
    "        self.reset_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.update_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.new_gate = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        \n",
    "        # Decay parameters\n",
    "        self.gamma_x = nn.Linear(input_size, input_size)\n",
    "        self.gamma_h = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        # Fully connected layer for output\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x, m, d, h):\n",
    "        # Decay mechanism for inputs\n",
    "        gamma_x = torch.exp(-torch.max(self.gamma_x(d), torch.zeros_like(d)))\n",
    "        x_hat = m * x + (1 - m) * gamma_x * x\n",
    "        \n",
    "        # Decay mechanism for hidden state\n",
    "        d_h = d.mean(dim=1, keepdim=True)\n",
    "        gamma_h = torch.exp(-torch.max(self.gamma_h(d), torch.zeros_like(d_h)))\n",
    "        h_hat = gamma_h * h\n",
    "        \n",
    "        # Concatenate inputs\n",
    "        combined = torch.cat((x_hat, h_hat), dim=1)\n",
    "        \n",
    "        # GRU gates\n",
    "        reset = torch.sigmoid(self.reset_gate(combined))\n",
    "        update = torch.sigmoid(self.update_gate(combined))\n",
    "        new = torch.tanh(self.new_gate(torch.cat((x_hat, reset * h_hat), dim=1)))\n",
    "        \n",
    "        # Compute new hidden state\n",
    "        h_new = (1 - update) * h + update * new\n",
    "        \n",
    "        # Output layer\n",
    "        out = self.fc(h_new)\n",
    "        return out, h_new"
   ],
   "id": "2bfb383c98669ac7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Hyperparameters\n",
    "input_size = 38  # The number of input features\n",
    "hidden_size = 64  # Size of the hidden state\n",
    "output_size = 1  # Size of the output\n",
    "num_epochs = 50\n",
    "batch_size = 16\n",
    "learning_rate = 0.001"
   ],
   "id": "f4835b82f9e12bed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize model, criterion, and optimizer\n",
    "model = GRUD(input_size, hidden_size, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "id": "94b407c8c4e1cf0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.eval()",
   "id": "ca61b039764e29e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(torch.tensor(X_tensor, dtype=torch.float32), \n",
    "                              torch.tensor(M_tensor, dtype=torch.float32), \n",
    "                              torch.tensor(Delta_tensor, dtype=torch.float32), \n",
    "                              torch.tensor(labels, dtype=torch.float32))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ],
   "id": "27811dae0ef273ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for batch in train_loader:\n",
    "    X_batch, M_batch, Delta_batch, labels_batch = batch\n",
    "    print(f'X_batch shape: {X_batch.shape}')\n",
    "    print(f'M_batch shape: {M_batch.shape}')\n",
    "    print(f'Delta_batch shape: {Delta_batch.shape}')\n",
    "    print(f'Labels_batch shape: {labels_batch.shape}')\n",
    "    break"
   ],
   "id": "74379dc3b08daf8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Training loop\n",
    "train_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for X_batch, M_batch, Delta_batch, y_batch in train_loader:\n",
    "        # Initialize hidden state\n",
    "        h = torch.zeros((X_batch.size(0), hidden_size))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        for t in range(X_batch.size(1)):\n",
    "            x_t = X_batch[:, t, :]\n",
    "            m_t = M_batch[:, t, :]\n",
    "            d_t = Delta_batch[:, t, :]\n",
    "            \n",
    "            output, h = model(x_t, m_t, d_t, h)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(output, y_batch)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')"
   ],
   "id": "96364e38c7c235e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), \n",
    "                             torch.tensor(M_test, dtype=torch.float32), \n",
    "                             torch.tensor(Delta_test, dtype=torch.float32), \n",
    "                             torch.tensor(y_test, dtype=torch.float32))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "1822d98c66a5d08f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot the training loss\n",
    "plt.figure()\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "5671b0e454bea215",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Evaluation on the test set\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "with torch.no_grad():\n",
    "    for X_batch, M_batch, Delta_batch, y_batch in test_loader:\n",
    "        # Initialize hidden state\n",
    "        h = torch.zeros((X_batch.size(0), hidden_size))\n",
    "        \n",
    "        # Forward pass\n",
    "        for t in range(X_batch.size(1)):\n",
    "            x_t = X_batch[:, t, :]\n",
    "            m_t = M_batch[:, t, :]\n",
    "            d_t = Delta_batch[:, t, :]\n",
    "            \n",
    "            output, h = model(x_t, m_t, d_t, h)\n",
    "        \n",
    "        # Convert output to binary prediction\n",
    "        predictions = (output >= 0.5).float()\n",
    "        total_correct += (predictions == y_batch).sum().item()\n",
    "        total_samples += y_batch.size(0)\n",
    "\n",
    "accuracy = total_correct / total_samples\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ],
   "id": "7a8447ec33081f51",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
